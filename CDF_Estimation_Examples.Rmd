---
title: "CDF Estimation"
author: "Michael Hornstein"
date: "11/20/2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(dplyr)
library(reshape2)
```

 

## Dvoretzky-Kiefer-Wolfowitz inequality and Confidence Bands

By the Dvoretzky-Kiefer-Wolfowitz inequality, 
\begin{equation} \label{DKW}
P\left( \sup_x \left | F(x) - \widehat{F}_n(x) \right | > \varepsilon \right) \leq 2 \exp( - 2n \varepsilon^2).
\end{equation}


The right-hand side of \eqref{DKW} is a decreasing function of $n$ and $\varepsilon$.  The larger the sample size $n$, the smaller the probaiblity that the error exceeds $\varepsilon$.  The larger the error threshold $\varepsilon$, the smaller the probability that the error exceeds $\varepsilon$ (i.e. a larger $\varepsilon$ is more permissive).  

Geometrically, the event 
\begin{equation} \label{supError}
\mathcal{E} := \left\{ \sup_x \left| F(x) - \widehat{F}_n(x) \right| \leq \varepsilon \right\}
\end{equation}
corresponds to the estimated function $\widehat{F}_n$ being within a band of width $\varepsilon$ above or below the function $F$.  Inequality \eqref{DKW} bounds the probability that $\mathcal{E}$ fails to occur.

```{r echo=FALSE}
# True CDF 
x.values <- seq(from=-3, to=3, length.out=1e3)
F.true <- pnorm(x.values)
# Estimated CDF 
n <- 30
sample.values <- rnorm(n)
F.hat <- ecdf(sample.values)(x.values)
df <- data.frame(x.values, F.true, F.hat)
alpha <- 0.05 
conf.band.width <- sqrt(log(2 / alpha) / (2 * n))
# Plot a band around 
ggplot(df, aes(x.values, F.hat)) + 
  geom_ribbon(aes(ymin=F.hat - conf.band.width, 
                  ymax=F.hat + conf.band.width), 
              fill="grey70") + 
  geom_line(col="red") + 
  geom_line(aes(x.values, F.true)) + 
  ylab("Cumulative Distribution Function") + 
  xlab("x") + 
  ggtitle("CDF with Confidence Band") + 
  annotate("text", x=0, y=0.5, label="")
```


As discussed in Chapter 7 of Wasserman's All of Statistics, we can use this result to form a confidence band around the empirical cdf.

For a fixed sample size $n$, we can find the value of $\varepsilon$ for which the probability bound in \eqref{DKW} is equal to $\alpha = 0.05$.  To do so, we solve the following equation for $\varepsilon$: 
$$
2\exp( - 2n \varepsilon^2) = \alpha.
$$
The solution is 
$$
\varepsilon = \sqrt{ \frac{\log(\alpha / 2)}{-2n} } = \sqrt{ \frac{\log(2 / \alpha)}{2n} }.
$$
It follows that 
$$
P\left( \sup_x \left | F(x) - \widehat{F}_n(x) \right | > \sqrt{ \frac{\log(2 / \alpha)}{2n} } \right) \leq \alpha.
$$

### Confidence Bands 

Inequality \eqref{DKW} is of the form 
\begin{equation} \label{confBandBound}
  P\left( \sup_x \left| F(x) - \widehat{F}_n(x) \right| > \varepsilon \right) \leq g(\varepsilon, n),
\end{equation}
where $g$ is a decreasing function of the sample size $n$ and the error threshold $\varepsilon$.  The event in \eqref{confBandBound} is that the true cdf $F$ does not lie within the confidence band of half-width $\varepsilon$.  For a given $n$, we can choose $\varepsilon$ so the probability is at most $\alpha$.  To choose $\varepsilon$, we solve $g(\varepsilon, n) = \alpha$ for $\varepsilon$.  Let's call the solution $\varepsilon_\alpha(n)$.  Plugging in $\varepsilon_\alpha(n)$ yields
$$
P\left( \sup_x \left| F(x) - \widehat{F}_n(x) \right| > \varepsilon_\alpha(n) \right) \leq g( \varepsilon_\alpha(n), n) = \alpha.
$$
A key point regarding \eqref{confBandBound} is that $g(\varepsilon, n)$ does not depend on the true cdf $F$.  The bound holds with the same probability regardless of the true unknown $F$.  This is what makes it possible to form the confidence band.  Regardless of the true cdf $F$, the event $\sup_x \left| F(x) - \widehat{F}_n \right| > \varepsilon$ has probability at most $\alpha$, so the confidence band contains the true $F$ with probability at least $1 - \alpha$.  

## Understanding the right-hand side of DKW inequality
The DKW inequality is not vacuous if 
$$
2 \exp(-2n \varepsilon^2) < 1,
$$ 
which is equivalent to 
$$ 
n > \frac{ \log(1/2) }{-2\varepsilon^2} = \frac{\log(2)}{2 \varepsilon^2}.
$$ 
The necessary order of growth of $n$ for the bound to not be vacuous is $n = \Omega(\varepsilon^{-2})$.  
```{r echo=FALSE}
eps.values <- seq(from=0.01, to=2, length.out=1e3)
n.values <- log(2) / (2 * eps.values^2)
par(mfrow=c(1, 2))
par(pty="s")
plot(eps.values, n.values, type="l", ylab="n", xlab="epsilon")
plot(eps.values, log10(n.values), type="l", 
     ylab="log10(n)", xlab="epsilon")
```

```{r}
n.values <- seq(from=10, to=75, by=10)
epsilon.values <- seq(from=0.21, to=1, length.out=8)
bound <- outer(X=n.values, Y=epsilon.values, 
               FUN=function(n, eps) 2 * exp(- 2 * n * eps^2))
df <- data.frame(n.values, bound)
# names(df)[2:ncol(df)] <- paste0(
#   "eps_", round(epsilon.values, 2))
names(df)[2:ncol(df)] <- round(epsilon.values, 2)
df.melted <- melt(df, id.vars="n.values", 
                  variable.name="epsilon")
ggplot(df.melted, aes(n.values, value, col=epsilon)) +
  geom_line() + 
  ggtitle("Probability Bound vs. Sample Size") + 
  xlab("n") + 
  ylab("Upper Bound on Probability") + 
  ylim(0, 1)
```


```{r}
x.values <- seq(from=-3, to=3, length.out=1e3)
n.values <- c(10, 20, 30, 40)
df <- data.frame(
  x = rnorm(sum(n.values)),
  n  = rep(n.values, times=n.values) %>% factor)

# Plot all four empirical cdf's on the same plot
ggplot(df, aes(x, col=n)) + stat_ecdf() + 
  stat_function(fun = pnorm, geom="line", lwd=1, 
                col="red") 

# Separate plot for each cdf
ggplot(df, aes(x)) + facet_wrap(~n) + stat_ecdf() + 
  stat_function(fun = pnorm, geom="line", lwd=1, 
                col="red") 

x.values <- seq(from=-3, to=3, length.out=1e3)
n.values <- c(10, 20, 30, 40)
df <- data.frame(
  x = rnorm(sum(n.values)),
  n  = rep(n.values, times=n.values) %>% factor) %>% 
  group_by(n) %>% 
  mutate(ecdf = ecdf(x)(sort(x)), 
         x.sorted = sort(x))
ggplot(df, aes(x.sorted, ecdf, col=n)) + geom_step()
```


## To Do 

* Fix the legend of the graph of probability vs. sample size 
* Plot confidence bands for different values of n

## Scratch Work 

This inequality states that with probability at least $1 - \alpha$, the function $\widehat{F}_n$ remains within a band of $F$ of width plus/minus $\varepsilon_\alpha(n)$, i.e. 
$$ 
  \lVert F - \widehat{F}_n \rVert_\infty \leq \varepsilon_\alpha(n) \qquad \text{with probability at least $1 - \alpha$},
$$ 
where $\lVert f(x) \rVert_\infty := \sup_x |f(x)|$ is the $\ell_\infty$ norm.  

$$
P\left( \sup_x \left| F(x) - \widehat{F}_n(x) \right| > \varepsilon(n) \right) \leq g( \varepsilon(n), n).
$$
In particular, for a fixed $\alpha \in [0, 1]$ (typically $\alpha = 0.05$), let's solve the equation $g(\varepsilon, n) = \alpha$ for $\varepsilon$ as a function of $n$, denoting the solution as $\varepsilon_\alpha(n)$.  Then 

Inequality \eqref{confBandBound} uses a fixed threshold $\varepsilon$ that does not depend on $n$.  But we could allow the threshold to depend on $n$ by applying the inequality using a threshold that depends on $n$, as follows: 

The function $g(\varepsilon, n)$ is a decreasing function of $n$, so as the sample size increases, the probability that the error exceeds a fixed threshold decreases.  Furthermore, note that $g(\varepsilon, n)$ is a decreasing function of $\varepsilon$, because as the threshold increases (i.e. gets more permissive), the probability that the error exceeds the threshold decreases.

See the link https://ggplot2.tidyverse.org/reference/stat_ecdf.html.

```{r}
n <- 1e2
x.values <- seq(from=-4, to=4, length.out=n) %>% sort
df <- data.frame(z.observed = rnorm(n), 
                 x.values, 
                 z.pdf = pnorm(x.values))
ggplot(df) + stat_ecdf(aes(z.observed)) + 
  geom_line(aes(x.values, z.pdf), col="red")
```




